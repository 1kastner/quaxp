{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Information\n",
    "__Section__: More complex transformations of text  \n",
    "__Goal__: Learn and understand the complex methods of text preprocessing such as stemming, negation handling, stopwords, ...  \n",
    "__Time needed__: 30 min  \n",
    "__Prerequisites__: None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex transformations of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Get example tweets\n",
    "tweet_1 = 'Hospitalizations from COVID-19 have increased nearly 90% and California officials say they could triple by Christmas. https://t.co/hrBnP04HnB'\n",
    "tweet_2 = 'Something for the afternoon slump / journey home / after school / cooking dinner ... a special 30 minute mix of cool Christmas tunes intercut with Christmas film samples and scratching @BBCSounds https://t.co/rHovIA3u5e'\n",
    "tweet_3 = 'This happened in Adelaide the other day. #koala #Adelaide https://t.co/vAQFkd5r7q'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negations and contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words such as \"can't\", \"don't\", in other words, words containing a negative contracted form, could be recognized by our algorithm, however, it is possible to make it simpler by removing the contracted form from the text (\"can not\", \"do not\"). A \"not\" is easier to interpret as it is a more frequent word than all the contracted forms.\n",
    "\n",
    "Of course, one can argue that, with a bag-of-words method, having two words instead of one, including one that is meaning the opposite of what we want to show, can be misleading (for example, \"not happy\" contains the word \"happy\", but means the opposite). This can be true in some case, so we always should have a good overview of the data and the problem we deal with, as always.  \n",
    "\n",
    "Depending on the methods of text processing might you use for your algorithm, you might have more or less use in replacing negations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of words are present for syntaxic purposes, but do not add any information for the task if we consider their meaning. Those words, such as \"a\", \"the\", \"for\", etc... (see the list below) should be removed for a more efficient analysis, otherwise they will probably have a high frequency and pollute the results of the bag-of-words method. This also allows the whole experiment to be ran faster with less information to treat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle} Advanced level\n",
    "In Python, some libraries come with a built-in list of stopwords, making it easy to remove them. For example, from the library [nltk.corpus](https://www.nltk.org/_modules/nltk/corpus.html), the class ``stopwords`` contains a list of stopwords that one can simply remove from the dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospitalizations from COVID-19 have increased nearly 90% and California officials say they could triple by Christmas. https://t.co/hrBnP04HnB\n",
      "['Hospitalizations', 'COVID-19', 'increased', 'nearly', '90', '%', 'California', 'officials', 'say', 'could', 'triple', 'Christmas', '.', 'https', ':', '//t.co/hrBnP04HnB']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(tweet_1)\n",
    "\n",
    "tokens = word_tokenize(tweet_1)\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        tokens.remove(token)\n",
    "        \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emojis can be a great source of information for sentiment analysis, and it would be a shame to simply discard them. Instead, we can use a mapping function, which gives a corresponding text translation for each emoji. With this example, the emoji \":-)\" will be translated into \"simple_smile\".\n",
    "\n",
    "A \"cheat sheet\" of emojis transformed into text can be seen [here](https://www.webfx.com/tools/emoji-cheat-sheet/).\n",
    "\n",
    "This emoji transformation has to be done before we remove all special characters, as we showed on the previous page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle} Advanced level\n",
    "A few resources exist about emoji transformation. Depending on how the emoji is represented in the text we want to process, we will use different functions.\n",
    "\n",
    "For example, the library [emoji](https://pypi.org/project/emoji/) offer a few functions to transform emojis to text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Stemming is a linguistic operation consisting of reducing the words to their stem, or root, by removing all language suffixes. The stem might not be a real word.\n",
    "+ Lemmatization is a similar operation which groups all words with the same root into one entity, the lemma, which is a real word.\n",
    "\n",
    "They often give the same result, but not always.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Words | Stemming | Lemmatization |\n",
    "|:-----|:---------|:--------------|\n",
    "| written | writ | write |\n",
    "| writing | writ | write |\n",
    "| gives | giv | give |\n",
    "| finally | final | final |\n",
    "| expected | expect | expect |\n",
    "| picky | pick | pick |\n",
    "\n",
    "This has the impact to consider all the words with the same root as synonyms, as they will be treated as a single word. This makes sense for an algorithm that is trying to detect the general idea of the text more than studying each word one by one. This operation is useful to reduce the number of different words in the data and therefore build a stronger model.\n",
    "\n",
    "A stemming algorithm is easier to write, as the word is only cut and no dictionnary or lookup table is needed, but it can sometimes give uncertain results: for example, the word ``given`` will be reduced to ``giv``, but the word ``gave`` will stay ``gav`` even though those two words have the same root and should be grouped together.  \n",
    "A lemmatization algorithm requires more preleminary resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle} Advanced level\n",
    "In the ``nltk`` library, we can use the algorithm [PorterStemmer](https://www.nltk.org/_modules/nltk/stem/porter.html), which uses a specific stemming algorithm, the Porter stemming algorithm.\n",
    "\n",
    "Try to change the words to see how other words change.\n",
    "\n",
    "Note that although the algorithm is said to be stemming, it gives real words and not cut versions of the root.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing - write\n",
      "gives - give\n",
      "expected - expect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "list_words = ['writing', 'gives', 'expected']\n",
    "\n",
    "for word in list_words:\n",
    "    print(word, '-', porter.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"959\"\n",
       "            height=\"309\"\n",
       "            src=\"https://blog.hoou.de/wp-admin/admin-ajax.php?action=h5p_embed&id=67\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15fe08dfa58>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://blog.hoou.de/wp-admin/admin-ajax.php?action=h5p_embed&id=67\", \"959\", \"309\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Kumar, Harish, 2018, Classification of Short Text Using Various Preprocessing Techniques: An Empirical Evaluation, Recent Findings in Intelligent Computing Techniques, pp. 19-30.\n",
    "- Arpita et al., 2020, Data Cleaning of Raw Tweets for Sentiment Analysis, 2020 Indo – Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)\n",
    "- Bitext.com, 2018, [What is the difference between stemming and lemmatization?](https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/), online, accessed 04.12.2020"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
